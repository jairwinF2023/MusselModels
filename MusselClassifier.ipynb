{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TsdY0mYd3MCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90ad9db-93b0-4542-efe2-b2afee5d959a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "metadata": {
        "id": "QqfRUGna5cCr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BinaryCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryCNN, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "\n",
        "        # Calculate the size after convolutions and pooling\n",
        "        # Input size: 256x256\n",
        "        # After first pool: 128x128\n",
        "        # After second pool: 64x64\n",
        "        # After third pool: 32x32\n",
        "        self.fc_input_dim = 64 * 32 * 32\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.fc_input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output layer for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, 3, 256, 256)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)  # 128x128\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # 64x64\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)  # 32x32\n",
        "\n",
        "        x = x.view(-1, self.fc_input_dim)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x  # Sigmoid for binary output"
      ],
      "metadata": {
        "id": "LJ44sZTM5rTZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "class LabelDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_filenames = os.listdir(image_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "\n",
        "        image = cv2.imread(img_name)\n",
        "\n",
        "        image = cv2.resize(image, (256, 256))\n",
        "\n",
        "        # Convert to PyTorch tensors and permute to (C, H, W)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Convert to tensor and permute channels if no transform\n",
        "            image = image.astype('float32') / 255.0\n",
        "            image = torch.from_numpy(image).permute(2, 0, 1)\n",
        "\n",
        "        # Assign label based on filename\n",
        "        filename_lower = self.image_filenames[idx].lower()\n",
        "        if \"alive\" in filename_lower:\n",
        "          label = 1\n",
        "        elif \"dead\" in filename_lower:\n",
        "          label = 0\n",
        "        else:\n",
        "          label = -1\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "Hc7ArNVA_7Wr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "model = BinaryCNN()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Directories\n",
        "image_dir = '/content/drive/My Drive/Research/Labeled Items'\n",
        "\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = LabelDataset(image_dir)\n",
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.float().to(device)  # Convert labels to float for BCELoss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).view(-1) # Remove extra dimensions\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/15] Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'binary_cnn.pth')"
      ],
      "metadata": {
        "id": "0evoDEXK6FDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b7b2e2-5aa9-4032-ae31-4fc4884ac8c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15] Training Loss: 1.5934\n",
            "Epoch [2/15] Training Loss: 0.7403\n",
            "Epoch [3/15] Training Loss: 0.6795\n",
            "Epoch [4/15] Training Loss: 0.6719\n",
            "Epoch [5/15] Training Loss: 0.6435\n",
            "Epoch [6/15] Training Loss: 0.6288\n",
            "Epoch [7/15] Training Loss: 0.5534\n",
            "Epoch [8/15] Training Loss: 0.4661\n",
            "Epoch [9/15] Training Loss: 0.3631\n",
            "Epoch [10/15] Training Loss: 0.3215\n",
            "Epoch [11/15] Training Loss: 0.1697\n",
            "Epoch [12/15] Training Loss: 0.1477\n",
            "Epoch [13/15] Training Loss: 0.1143\n",
            "Epoch [14/15] Training Loss: 0.0925\n",
            "Epoch [15/15] Training Loss: 0.1013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "\n",
        "image_dir = '/content/drive/My Drive/Research/Train Label'\n",
        "\n",
        "trainset = LabelDataset(image_dir)\n",
        "dataloader = DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in dataloader:\n",
        "          images, labels = images.to(device), labels.float().to(device)\n",
        "          outputs = model(images).view(-1)\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss += loss.item() * images.size(0)\n",
        "\n",
        "          # Calculate accuracy\n",
        "          preds = (outputs >= 0.5).float()\n",
        "          correct += (preds == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(dataloader.dataset)\n",
        "    accuracy = correct / len(dataloader.dataset) * 100\n",
        "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {accuracy:.2f}%\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd7WDj7-97eu",
        "outputId": "d89733be-8774-4ec8-d975-cb4f4d5c9aa2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3550 | Validation Accuracy: 91.67%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}